{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from etl.canon import build_training_data\n",
    "\n",
    "# Path to the most recent diagnostic dataset\n",
    "project_root = Path.cwd().parent\n",
    "input_file = project_root / \"output\" / \"2025-11-03\" / \"diagnostic_dataset_2025-11-03_20-59-20.json\"\n",
    "\n",
    "print(f\"Building training data from: {input_file}\\n\")\n",
    "stats = build_training_data(str(input_file))\n",
    "\n",
    "print(f\"✓ Created training data from {stats['total']} samples\")\n",
    "print(f\"  - Train: {stats['train']} samples\")\n",
    "print(f\"  - Val: {stats['val']} samples\")\n",
    "print(f\"  - Test: {stats['test']} samples\")\n",
    "print(f\"\\n✓ Vocabulary sizes:\")\n",
    "print(f\"  - Symptoms: {stats['vocab_sizes']['symptoms']}\")\n",
    "print(f\"  - Families: {stats['vocab_sizes']['families']}\")\n",
    "print(f\"  - Subtypes: {stats['vocab_sizes']['subtypes']}\")\n",
    "print(f\"  - Brands: {stats['vocab_sizes']['brands']}\")\n",
    "print(f\"  - Diagnostics: {stats['vocab_sizes']['diagnostics']}\")\n",
    "print(f\"\\n✓ Output files written to: {stats['output_dir']}/\")\n",
    "print(\"  - train.jsonl, val.jsonl, test.jsonl\")\n",
    "print(\"  - vocabs.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddaa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e6fe3",
   "metadata": {},
   "source": [
    "## Optional: Run ETL Pipeline\n",
    "\n",
    "Uncomment and run the cell below if you need to rebuild the training data from raw diagnostic dataset:\n",
    "\n",
    "```python\n",
    "# Example: Build training data from diagnostic dataset\n",
    "# INPUT_FILE = \"../../output/2025-11-03/diagnostic_dataset_2025-11-03_20-59-20.json\"\n",
    "# stats = build_training_data(INPUT_FILE, output_dir=\"data\")\n",
    "# \n",
    "# print(f\"✓ Created training data from {stats['total']} samples\")\n",
    "# print(f\"  - Train: {stats['train']} | Val: {stats['val']} | Test: {stats['test']}\")\n",
    "# print(f\"  - Vocab sizes: {stats['vocab_sizes']}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89650763",
   "metadata": {},
   "source": [
    "## Load Processed Data and Setup Dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77142974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import HVACDataset\n",
    "\n",
    "# Define paths (data files are in etl/data/)\n",
    "VOCAB_PATH = \"etl/data/vocabs.json\"\n",
    "TRAIN_PATH = \"etl/data/train.jsonl\"\n",
    "VAL_PATH   = \"etl/data/val.jsonl\"\n",
    "TEST_PATH  = \"etl/data/test.jsonl\"\n",
    "\n",
    "# Load vocabs and calculate dimensions\n",
    "v = json.load(open(VOCAB_PATH))\n",
    "\n",
    "SYM_DIM = len(v[\"symptom2id\"])\n",
    "FAM_DIM = len(v[\"family2id\"])\n",
    "SUB_DIM = len(v[\"subtype2id\"])\n",
    "BR_DIM  = len(v[\"brand2id\"])\n",
    "INPUT_DIM = SYM_DIM + FAM_DIM + SUB_DIM + BR_DIM\n",
    "NUM_CLASSES = len(v[\"diag2id\"])\n",
    "\n",
    "print(f\"INPUT_DIM={INPUT_DIM}  NUM_CLASSES={NUM_CLASSES}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_ds = HVACDataset(TRAIN_PATH, v)\n",
    "val_ds   = HVACDataset(VAL_PATH, v)\n",
    "test_ds  = HVACDataset(TEST_PATH, v)\n",
    "\n",
    "print(f\"rows: train={len(train_ds)} val={len(val_ds)} test={len(test_ds)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0, pin_memory=False)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ce65f",
   "metadata": {},
   "source": [
    "## Create PyTorch Datasets and DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f4766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIM=132  NUM_CLASSES=12\n"
     ]
    }
   ],
   "source": [
    "# Get a batch and inspect shapes\n",
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape, xb.dtype, yb.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db49d3c",
   "metadata": {},
   "source": [
    "## Inspect Batch Shape and Dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: train=24 val=7 test=4\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5984a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 132]), torch.Size([24, 12]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480b9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

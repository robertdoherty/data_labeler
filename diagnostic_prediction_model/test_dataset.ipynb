{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82fe003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training data from: /Users/robertdoherty/Desktop/Playground/Motherboard/road_runner/output/2025-11-03/diagnostic_dataset_2025-11-03_20-59-20.json\n",
      "\n",
      "✓ Created training data from 35 samples\n",
      "  - Train: 24 samples\n",
      "  - Val: 7 samples\n",
      "  - Test: 4 samples\n",
      "\n",
      "✓ Vocabulary sizes:\n",
      "  - Symptoms: 93\n",
      "  - Families: 11\n",
      "  - Subtypes: 16\n",
      "  - Brands: 12\n",
      "  - Diagnostics: 12\n",
      "\n",
      "✓ Output files written to: /Users/robertdoherty/Desktop/Playground/Motherboard/road_runner/diagnostic_prediction_model/etl/data/\n",
      "  - train.jsonl, val.jsonl, test.jsonl\n",
      "  - vocabs.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from etl.canon import build_training_data\n",
    "\n",
    "# Path to the most recent diagnostic dataset\n",
    "project_root = Path.cwd().parent\n",
    "input_file = project_root / \"output\" / \"2025-11-03\" / \"diagnostic_dataset_2025-11-03_20-59-20.json\"\n",
    "\n",
    "print(f\"Building training data from: {input_file}\\n\")\n",
    "stats = build_training_data(str(input_file))\n",
    "\n",
    "print(f\"✓ Created training data from {stats['total']} samples\")\n",
    "print(f\"  - Train: {stats['train']} samples\")\n",
    "print(f\"  - Val: {stats['val']} samples\")\n",
    "print(f\"  - Test: {stats['test']} samples\")\n",
    "print(f\"\\n✓ Vocabulary sizes:\")\n",
    "print(f\"  - Symptoms: {stats['vocab_sizes']['symptoms']}\")\n",
    "print(f\"  - Families: {stats['vocab_sizes']['families']}\")\n",
    "print(f\"  - Subtypes: {stats['vocab_sizes']['subtypes']}\")\n",
    "print(f\"  - Brands: {stats['vocab_sizes']['brands']}\")\n",
    "print(f\"  - Diagnostics: {stats['vocab_sizes']['diagnostics']}\")\n",
    "print(f\"\\n✓ Output files written to: {stats['output_dir']}/\")\n",
    "print(\"  - train.jsonl, val.jsonl, test.jsonl\")\n",
    "print(\"  - vocabs.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddaa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e6fe3",
   "metadata": {},
   "source": [
    "## Optional: Run ETL Pipeline\n",
    "\n",
    "Uncomment and run the cell below if you need to rebuild the training data from raw diagnostic dataset:\n",
    "\n",
    "```python\n",
    "# Example: Build training data from diagnostic dataset\n",
    "# INPUT_FILE = \"../../output/2025-11-03/diagnostic_dataset_2025-11-03_20-59-20.json\"\n",
    "# stats = build_training_data(INPUT_FILE, output_dir=\"data\")\n",
    "# \n",
    "# print(f\"✓ Created training data from {stats['total']} samples\")\n",
    "# print(f\"  - Train: {stats['train']} | Val: {stats['val']} | Test: {stats['test']}\")\n",
    "# print(f\"  - Vocab sizes: {stats['vocab_sizes']}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89650763",
   "metadata": {},
   "source": [
    "## Load Processed Data and Setup Dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77142974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIM=132  NUM_CLASSES=12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import HVACDataset\n",
    "\n",
    "# Define paths (data files are in etl/data/)\n",
    "VOCAB_PATH = \"etl/data/vocabs.json\"\n",
    "TRAIN_PATH = \"etl/data/train.jsonl\"\n",
    "VAL_PATH   = \"etl/data/val.jsonl\"\n",
    "TEST_PATH  = \"etl/data/test.jsonl\"\n",
    "\n",
    "# Load vocabs and calculate dimensions\n",
    "v = json.load(open(VOCAB_PATH))\n",
    "\n",
    "SYM_DIM = len(v[\"symptom2id\"])\n",
    "FAM_DIM = len(v[\"family2id\"])\n",
    "SUB_DIM = len(v[\"subtype2id\"])\n",
    "BR_DIM  = len(v[\"brand2id\"])\n",
    "INPUT_DIM = SYM_DIM + FAM_DIM + SUB_DIM + BR_DIM\n",
    "NUM_CLASSES = len(v[\"diag2id\"])\n",
    "\n",
    "print(f\"INPUT_DIM={INPUT_DIM}  NUM_CLASSES={NUM_CLASSES}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "208b6818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: train=24 val=7 test=4\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_ds = HVACDataset(TRAIN_PATH, v)\n",
    "val_ds   = HVACDataset(VAL_PATH, v)\n",
    "test_ds  = HVACDataset(TEST_PATH, v)\n",
    "\n",
    "print(f\"rows: train={len(train_ds)} val={len(val_ds)} test={len(test_ds)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0, pin_memory=False)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ce65f",
   "metadata": {},
   "source": [
    "## Create PyTorch Datasets and DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "138f4766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 132]), torch.Size([24, 12]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a batch and inspect shapes\n",
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape, xb.dtype, yb.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db49d3c",
   "metadata": {},
   "source": [
    "## Inspect Batch Shape and Dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: train=24 val=7 test=4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiagnosticClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Wide + Deep over tabular vector x.\n",
    "    Expects batch[\"x\"] -> FloatTensor [B, INPUT_DIM]\n",
    "    If training with labels:\n",
    "      - soft labels: batch[\"y\"] -> FloatTensor [B, C] (rows sum ~1)\n",
    "      - hard labels: batch[\"y\"] -> LongTensor [B] (class ids)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes, hidden=(128,), drop=0.1, use_soft_labels=True):\n",
    "        super().__init__()\n",
    "        # Wide (linear)\n",
    "        # This is a linear classifier head: it multiplies your input features by a weight matrix and adds a bias to produce class scores (logits). \n",
    "        # Think “quick memory” from each single feature directly to each diagnosis.\n",
    "        self.wide = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "        # Deep (MLP ending in num_classes)\n",
    "        layers, dims = [], [input_dim] + list(hidden) # dims is the width of each deep layer: start with input_dim, then the hidden sizes (e.g., [input_dim, 128]).\n",
    "        for i in range(len(dims)-1): # The loop builds blocks: Linear → ReLU → Dropout from dims[i] to dims[i+1]\n",
    "            layers += [nn.Linear(dims[i], dims[i+1]), nn.ReLU(), nn.Dropout(drop)]\n",
    "        layers += [nn.Linear(dims[-1], num_classes)]\n",
    "        self.deep = nn.Sequential(*layers) # self.deep is the “combination brain” that learns feature interactions (e.g., symptom A and symptom B with brand C).\n",
    "\n",
    "        # Loss choice -- criterion is the loss function (how wrong we are)\n",
    "        self.use_soft_labels = use_soft_labels \n",
    "        self.criterion_ce = nn.CrossEntropyLoss() # CrossEntropy for hard labels (one class id per example).\n",
    "        self.criterion_kl = nn.KLDivLoss(reduction=\"batchmean\") # KLDiv for soft labels (a probability vector per example). Feed log-softmax(logits) against your probability targets\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch[\"x\"].float()               # [B, INPUT_DIM]\n",
    "        logits = self.wide(x) + self.deep(x) # [B, C]\n",
    "        return logits\n",
    "\n",
    "    # This is the loss function. It computes the loss between the model's logits and the true labels.\n",
    "    def compute_loss(self, logits, y):\n",
    "        if self.use_soft_labels:\n",
    "            # y: soft probs [B, C]\n",
    "            return F.kl_div(F.log_softmax(logits, dim=-1), y, reduction=\"batchmean\") # Soft labels (y is probs [B,C] that sum to ~1): KLDiv with log_softmax(logits).\n",
    "        else:\n",
    "            # y: class ids [B]\n",
    "            return self.criterion_ce(logits, y) # Hard labels (y is class ids [B]): CrossEntropy on logits directly.\n",
    "\n",
    "    # We always use the combined logits (wide+deep). The only switch is which loss matches your label format:\n",
    "    def step(self, batch):\n",
    "        logits = self.forward(batch)\n",
    "        loss = None\n",
    "        if \"y\" in batch and batch[\"y\"] is not None:\n",
    "            loss = self.compute_loss(logits, batch[\"y\"])\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_vocabs(cls, vocabs: dict, **kw):\n",
    "        input_dim = (len(vocabs[\"symptom2id\"])\n",
    "                     + len(vocabs[\"family2id\"])\n",
    "                     + len(vocabs[\"subtype2id\"])\n",
    "                     + len(vocabs[\"brand2id\"]))\n",
    "        num_classes = len(vocabs[\"diag2id\"])\n",
    "        return cls(input_dim=input_dim, num_classes=num_classes, **kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5984a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 132]), torch.Size([24, 12]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480b9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
